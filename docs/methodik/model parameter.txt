MODELLPARAMETER
--------------

Than et al.:
"For all tests,
we set a seed and set the temperature parameter to 0 to minimize output vari
ation and to enhance reliability and reproducibility, and we set the context
window to the maximum allowed for each model."
--------------------------------------------------------------------------------
Törnberg: 
"Tweaking these settings are important to improve reliability and desirability of 
responses, and it may take some experimentation to figure out the appropriate settings for your 
use cases. The following list shows some common settings you may come across when using 
LLMs: 
• Max Length – Sets the maximal number of tokens the model generates. Specifying a 
max length allows you to control costs, and prevent long or irrelevant responses. 
• Temperature – The temperature parameter controls how random the model output is, 
essentially increasing the weights of all other possible tokens. Low temperature leads to 
more deterministic results, while high temperature leads to more randomness, that is, 
more diverse or creative outputs. For data annotation, a lower temperature is usually 
recommended, such as 0. 
• Top-P: Adjusts the range of considered tokens. A low Top P ensures precise, confident 
responses, while a higher value promotes diversity by including less likely tokens. For 
data annotation, a lower Top-P is usually recommended, such as 0.2 to 0.4. If using Top
P, your temperature must be above 0. 
• Top-K: The top-k parameter limits the model’s predictions to the top-k most probable 
tokens. By setting a value for top-k, you can thereby limit the model to only considering 
the most likely tokens. 
Your parameters must always be explicitly specified – even if they are the default parameters – 
as this is necessary for reproducibility."

--------------------------------------------------------------------------------

ELLMER https://ellmer.tidyverse.org/reference/params.html

temperature
Temperature of the sampling distribution.

top_p
The cumulative probability for token selection.

top_k
The number of highest probability vocabulary tokens to keep.

frequency_penalty
Frequency penalty for generated tokens.

presence_penalty
Presence penalty for generated tokens.

seed
Seed for random number generator.

max_tokens
Maximum number of tokens to generate.

log_probs
Include the log probabilities in the output?

stop_sequences
A character vector of tokens to stop generation on.

reasoning_effort, reasoning_tokens
How much effort to spend thinking? ressoning_effort is a string, like "low", "medium", "high". reasoning_tokens is an integer, giving a maximum token budget. Each provider only takes one of these two parameters.

--> also alle spezifizieren.... 
--------------------------------------------------------------------------------
Parameter meta-llama/Llama-3.1-8B-Instruct (https://developers.cloudflare.com/workers-ai/models/llama-3.1-8b-instruct/)
 
max_tokens; default 256 
The maximum number of tokens to generate in the response.

tempereature; default 0.6 / min 0 max 5
Controls the randomness of the output; higher values produce more random results.

top_p => default = NULL / min 0 max 2
Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.

top_k => default = 0 / min 1 max 50
Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.

seed; min 1 max 9999999999
Random seed for reproducibility of the generation.

repetition_penatly =>default = NULL / min 0 max 2
Penalty for repeated tokens; higher values discourage repetition.

frequency_penalty => default = NULL / min 0 max 2
Decreases the likelihood of the model repeating the same lines verbatim.

presence_penalty > default = NULL / min 0 max 2
Increases the likelihood of the model introducing new topics.


context window = total nr of tokens model can consider at once (input and output tokens)
context window = 7968 tokens
1 Token = 0.75 Wörter (englisch) 


=> https://www.vellum.ai/llm-parameters/max-tokens?utm_source=google&utm_medium=organic
--------------------------------------------------------------------------------
PARAMETER CLASSIFIER:

temperature = 0
["Low temperature leads to 
more deterministic results, while high temperature leads to more randomness, that is, 
more diverse or creative outputs. For data annotation, a lower temperature is usually 
recommended, such as 0." Thönberg et al.]

top_p / top_k 
["For data annotation, a lower Top-P is usually recommended, such as 0.2 to 0.4. If using Top
P, your temperature must be above 0." aus Thönberg et al.]
["Top P can be mixed with temperature, but that is typically not advisable. While temperature is typically used over Top P, Top P is useful when token options aren’t as long-tailed."
https://www.vellum.ai/llm-parameters/temperature?utm_source=google&utm_medium=organic
] => weil temp = 0 ist top_p und top_k egal, wird ignoriertn // muss nicht in list spezifiziert werden. 

seed setzen wichtig!


max_token: 
bisher 300; 
ausgabe JSON:
"{\"code\": \"Wirtschaftlicher Nutzen\", \"reasoning\": \"Der Artikel erwähnt, dass dank der Personenfreizügigkeit es für Studierende viel unkomplizierter ist, im jeweiligen Land eine Niederlassungsbewilligung zu bekommen und ohne bürokratische Formalitäten einzureisen. Dies suggeriert, dass die Personenfreizügigkeit einen wirtschaftlichen Nutzen für Studierende bringt.\"}"
==> ugf. 63 Tokens..
Ich würde Länge der Begründung im User Pormpt definieren (kurz halten); max_tokens als Kostenbremse falls etw. schiefläuft. Doch etwas höher als um die 70 Tokens, falls Begründungen länger werden. Sonst stoppt es nachdem Limit erreicht ist und JSON ist kapputt...
also... Limit 300 ist ok; Kostenunterschied ist immer noch miniiiiiiiiiiiiiiiiiiiim und eigentlich stoppts ja vorher
denke ist nicht mega relevant auch keinen Einfluss auf die Klassifizieurng selbst

penalties = not necessary/soll keine Bestrafung für Wortwiederholungen/Themenwiederholungen geben, sonst Verfälschung. Ist default auf Null/deaktiviert, kann ignoriert werden. 
--------------------------------------------------------------------------------
PARAMETER INSTRUCTOR (GENERATE)

temperature
[experimentieren, versch. versuchen, aber tendentiell eher höher weil einfach generieren, kreativ, ... aber niedriger weil vorgaben codebook dürfen nicht ingnoriert werden... ausrbobierem]
vergl. 0, 0.3, 0.6

top_p
[auch eher höher, versuch mal ganz hoch, mittel tief]
...

top_k 
[eigentlich schon durch top_p gegeben? evtl einfach default (NULL) belassen]
...

seed
[um parameter zu testen, danach kann's raus]

penalties: Begriffe müssen ja evtl. wiederholt werden, würd ich weglassen
--------------------------------------------------------------------------------
PARAMETER INSTRUCTOR (OPTIMIZE)

temperature
[auch eher paar Versuche machen, aber generell rel. tief (so 0.2? bisschen kreativ aber gezielt feedback?)
vergl. 0, 0.2, 0.4

top_p
[auch bisschen höher, aber tiefer als generate; weil schon rel. gezielt Feedback einbauen]

top_k
[siehe oben]

penalties: würd ich auch eher sein lassen, sonst gefahr, dass es von vorgaben abweicht..
--------------------------------------------------------------------------------