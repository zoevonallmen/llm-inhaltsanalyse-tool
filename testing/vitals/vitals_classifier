Ausgangslage: Erklärung zum Vorgehen: 
-------------------------------------

Zu diesem Zeitpunkt: Ist das Modell (meta-llama/Llama-3.1-8B-Instruct) eine gute Wahl für die Klassifizierungsaufgaben (Classifier)? / Ist es zuverlässig?

1) Liefert das Modell zuverlässig strukturierte Ergebnisse (gültiges JSON-Format, immer mit Code und Begründung/Reasoning)
-> Mehrere Artikel testen

2) Beeinträchtigt die Länge von Prompt und/oder Artikel/Text-Input das Modell? (Stabilität) 
-> kurze vs. lange Artikel testen; kure vs. lange Prompt testen
-> beurteilen = JSON-Format, Begründung/Reasoning, Latenz

3) Beeinträchtigt die Anzahl Artikel/Text-Inputs das Modell?
- 1 Artikel vs. 20-30 Artikel (loop)
-> Ausfälle? Einfluss auf Klassifizierung? 

(4) Falls es nicht funktioniert, wann/wie?)

Vitals-package wurde nicht verwendet. Evtl. hab ich das package noch nicht ganz verstanden, aber ich fand, dass wir aktuell ja nur die Zuverlässigkeit und Stabilität des Modells testen wollen (sprich gibt es Einbrüche bei längeren Prompts oder Artikeln oder einer grösseren Anzahl von Testen) und nicht die Genauigkeit der Codierungen oder wie sich die Promptoptimierung darauf auswirkt, da die Prompt ja mittels LLM optimiert werden soll
Ich fände es aber sinnvoll, das vitals-Package später einzuführen, zb zur Gesamtevaluation des Tools und ggf. (wenn das überhaupt möglich ist) direkt ins Tool zu integrieren, um zb Modelloutput mit manuell codierten Testartikeln abzugleichen (quasi die manuelle Codierung als "target"). Dadurch könnte auch gleich der Einfluss der Prompt-Optimierung (vom Instructor) auf die Klassifizerungsgenauigkeit (Übereinstimmung der Codes, nicht Reasonings) untersucht werden. Oder einfach am Schluss für meine Gesamtevaluation des Tools. 


Ergebnisse; Fazit; Fragen:
--------------------------
Auch beim Loop von 50 Artikeln (25 kurz und 25 lang) mit der langen/komplizierteren Prompt sind die Ergebnisse strukturiert & im verlangten Format. Der Code ist beim selben Artikel jeweils identisch, die Begründungen sind nachvollziehbar, es wird nichts halluziniert.

Zeit für die Klassifizierung pro Artikel (bei 50)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.542   2.826   3.568   4.000   5.046   6.315 
  
==> ich würde sagen, das ist ein gutes Resultat, zeig, dass das Modell so weiterhin genutzt werden kann (oder?)

Das einzige Problem: das Modell muss im Loop (siehe vitals_classifier.R) für jeden Artikel neu initialisiert werden. Habe es ohne versucht, dann schlägt die Klassifizierung für jeden Artikel auch nach 3 Versuchen fehl....
-> Ist das ein Problem? Hat das Auswirkungen auf die Kosten?

Und: bei der Classifier Funktion (classifier.R) hab ich eine Retry-Logik eingebaut; ein Aufruf wird bis zu 3 Mal wiederholt, mit Sys.sleep(5) dazwischen. Das hab ich bei einem früheren Projekt so gemacht, aber war nicht mehr sicher ob das so ok ist?




















--------------------------------------------------------------------------------
Ergebnisse (sehr detailliert; Schritt für Schritt)

1) Artikellänge (lang= 9043 Zeichen / kurz = 1439 Zeichen) & Promptlänge(-komplexität)
-----------------------------------------------------------------------------
Kurze Prompt x kurzer Artikel:
Format gut, Begründung verständlich. 
> response$success
[1] TRUE
> response$code
[1] "Wirtschaft"
> response$reasoning
[1] "Der Text handelt hauptsächlich von der Wirtschaft und der Auswirkung der SVP-Initiative auf den Studentenaustausch und die Personenfreizügigkeit. Economiesuisse wird als Wirtschaftsverband erwähnt und ihre Kampagne zur Abstimmung wird analysiert. Der Text befasst sich auch mit den Auswirkungen auf den Bildungsvertrag und die bilateralen Verträge, was ein wichtiger Aspekt der Wirtschaftspolitik ist."
--
Kurze Prompt x langer Artikel:
Format gut, Begründung verständlich
> response$success
[1] TRUE
> response$code
[1] "Sicherheit"
> response$reasoning
[1] "Der Text handelt hauptsächlich von der Sicherheit und der Auswirkung der Personenfreizügigkeit auf die Schweiz. Die Diskussion um die Zuwanderung und die Sozialleistungen für EU-Bürger wird analysiert und die Bedenken um die Sicherheit und die Finanzierung der Sozialversicherungen werden hervorgehoben. Die Zunahme bei den Totalisierungen und die Diskussion um die Meldepflicht für stellenlose EU-Bürger sind wichtige Aspekte der Sicherheitsdiskussion."
---
kurzer Artikel, lange/komplexe Prompt
Format stimmt immer noch; Code und Begründung wie bei der kurzen Prompt 
### Note: evtl. ein Tokenmax. beim Reasoning einbauen? was wäre sinnvoll...
> response$success
[1] TRUE
> response$code
[1] 4 (Sicherheit)
> response$reasoning
[1] "Der Artikel diskutiert die Zunahme von EU-Bürgern, die in der Schweiz Arbeitslosengeld beziehen, obwohl sie seit weniger als einem Jahr hier leben. Die Behörden bemühen sich, Missbräuche zu verhindern, aber die Zahlen zeigen, dass die Situation sich verschlechtert. Die Artikel spricht von 'Armutsmigration' und 'Missbrauch' und betont, dass die Schweiz Zeit hat, Gegenmassnahmen zu ergreifen. Die Justizministerin Simonetta Sommaruga prüft eine Meldepflicht für stellenlose EU-Bürger, um die Personenfreizügigkeit nicht dazu zu nutzen, in einem fremden Land an grosszügige Sozialleistungen zu gelangen. Der Artikel suggeriert, dass die Einwanderung die Sozialwerke belastet und dass die Schweiz Zeit hat, Gegenmassnahmen zu ergreifen. Daher ist der dominante Frame 'Sicherheit'."
> response$raw
```json
{
  "code": 4,
  "reasoning": "Der Artikel diskutiert die Zunahme von EU-Bürgern, die in der Schweiz Arbeitslosengeld 
beziehen, obwohl sie seit weniger als einem Jahr hier leben. Die Behörden bemühen sich, Missbräuche zu 
verhindern, aber die Zahlen zeigen, dass die Situation sich verschlechtert. Die Artikel spricht von 
'Armutsmigration' und 'Missbrauch' und betont, dass die Schweiz Zeit hat, Gegenmassnahmen zu ergreifen. 
Die Justizministerin Simonetta Sommaruga prüft eine Meldepflicht für stellenlose EU-Bürger, um die 
Personenfreizügigkeit nicht dazu zu nutzen, in einem fremden Land an grosszügige Sozialleistungen zu 
gelangen. Der Artikel suggeriert, dass die Einwanderung die Sozialwerke belastet und dass die Schweiz Zeit
hat, Gegenmassnahmen zu ergreifen. Daher ist der dominante Frame 'Sicherheit'."
}

---

Problem: Wenn ich das Environment zwischen den Läufen nicht lösche, funktioniert es nach 2-3 Mal nicht mehr: Error 400: The input is longer than the model's context length (Tokenmax.überschreitung)


LOOP: 10 Artikel (5 kurz, 5 lang) --> Error 400; Tokenmax. überschritten...
-> hf_model für jeden Artikel neu initialisieren => funktioniert mit kurzer Prompt, auch im JSON Format und Code bleibt gleich/Reasoning ist stabil (ist das eine "stabile"" Lösung?? oder Problem für Modell) oder kostet das mehr?? wenn ich's rausnehme failed jeder artikel nach 3 tries 

bei Loop & lange Prompt: Error in `req_perform()`: ! HTTP 504 Gateway Timeout. -> retry?
Retry & sys.sleep; funktioniert. 
Ergebnisse Loop x lange Prompt:
Format stimmt, wird korrekt extrahiert. Kurzer Artikel: Code und Begründung bleibt gleich wie bei den vorherigen Versuchen; langer Artikel: anderer Code... aber Begrüdnung ist verständlich; nichts Halluziniert. => grundsätzlich ok; kann man über Prompt Engineering verbessern würd ich sagen...


für 50 Artikel (25 kurz, 25 lang):
Alle im korrekten Format; Codes gleich & Begründungen ähnlich wie mit 10 Artikel Loop. 
cat("Success:", mean(loop_50_articles_testing_classifier_longPrompt_incl_latency$success), "\n")
Success: 1 
> print(summary(loop_50_articles_testing_classifier_longPrompt_incl_latency$latency_s))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.075   2.382   3.091   3.156   3.793   4.901 
==> lange Artikel eher mehr Zeit, kurze weniger. 

Sollte ich noch etwas einbauen womit Artikel getrennt werden können, wenn sie zu gross sind? Oder doch nicht weils jetzt funktioniert?